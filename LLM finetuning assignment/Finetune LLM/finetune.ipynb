{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE5GJ6s7y0Xo"
      },
      "source": [
        "### Finetuning large models using peft adapters, transformers & bitsandbytes\n",
        "\n",
        "In this colab, I have finetuned large language models using the very recent peft library and bitsandbytes for loading large models in 8-bit. The fine-tuning method will rely on a recent method - Low Rank Adapters (LoRA), instead of finetuning the entire model we just have to fine-tune these adapters and load them properly inside the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfBzP8gWzkpv"
      },
      "source": [
        "Install requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otj46qRbtpnd",
        "outputId": "bf809de3-22f5-4737-b7de-87ab93390651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOtwYRI3zzXI"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "25ad5e5af8704b0f8e44f58ab6226f21",
            "5b161f261318466ba2d3443a919ede60",
            "a22546f488c740bd97c77be366b5e067",
            "702de87eb807415294b5c0dcf97a78b1",
            "df9b59b750084488a0c050d1f6ca1712",
            "e8a5c49aa9fb4892a7d984138cb4bbd2",
            "b6d48b723101448f847ef41f21d7a3e8",
            "bb6f242642754ded845e08908129b502",
            "ab63944735694805b6c0451805c790d6",
            "42f7ece1116f42ba9df9067f2ae7754d",
            "bf29d22e0007492ea540eac839f816b0",
            "9038797e5e0941f187a8161f17595d3c",
            "cee9cc70c6a74164b37043a1a109f8a7",
            "079d7e0fdab3431faf2b5d3f907529bf",
            "7a6c2a071c5a4106befef48e06fe6ee4",
            "135bb280c1284d978d8abaa7c609df11",
            "492a5f065ac4481bbd1a15b071d4307b",
            "8f465bb6d1834951bae5538e939fd574",
            "bc68d29b7c38430cb6273777d10b676c",
            "b9792fcee3d34d508d86bd71e333e245",
            "4cc907d16f0045028663c479ee50047f",
            "a60c33cfd9be490da121f525f29efd3d",
            "30abe82956fb4c7dad56816e24c2664f",
            "d3baea3c774f4cac9c1e6906ebfdefe7",
            "58d87ed3c5d34651922738055171b7ad",
            "0ec4c76f1fb1438691df5cdd2f5a6839",
            "2d00221df7474ed28b7f5de5a984743e",
            "12e5008cb7d6490180f9c65085e47318",
            "a4a522d7bd054da7b22b36c85f2d87cc",
            "8e7c661fa73b43808f9be26cb857f01d",
            "cf97e2f0c59f4786aa4f6c71a7b2bf34",
            "3484e98610da4c92b8f1f3f73d304dea",
            "bad9b218e43e4993974ad1819837b901",
            "31c06feb84bc44e586ea0fb44e14bcbe",
            "02a97ae141844beb985a4a90455c0ea7",
            "e82b044b00734a7a9226af71b7d922ec",
            "a3d2ede1618d43dfadf3c3442e4a241f",
            "c445b00239f84155aa984d75ec4898a6",
            "7a707014a5e04941bd25a8e9c59d97d7",
            "270dce9cea164bd09243f4a5420380b8",
            "049e9314c77d46d797f261e6756bc288",
            "bfc06bc914544d91b1544b5a4ab0970f",
            "a1887107235a47df90101477a73214a5",
            "499eceb765c747dc87e8002b76788aad",
            "a0a1df2f6b08469a8e6241dbda85f284",
            "e29f6e75584149ff85a2fca6d215fb03",
            "bb47979276524167a29738155374df8f",
            "e1ca4ee93a6f4d0caee230851055a91e",
            "d368074383874d6182aa9d8cdf524c34",
            "cf52d7da2da2443aae13148c90778666",
            "9fa9f13a3dd14744ba58e0b08cbdb550",
            "680793ec7130498783d4d07d128b07b5",
            "4192c0f9bcca4a86a4023c9ee452cf64",
            "b54f6d5065954c059ee91650713881dc",
            "5397dd65326044ef94b19a3ae348b466",
            "5868fca753f24a5580cb9e506dd78ce8",
            "60f3e43e75b84d8aaba8fa9ddb0ec0ce",
            "27622384b8484bac9999cb566c8d9c62",
            "c4f8602ee2b24d16a7fc8cad7fae4176",
            "f3e8003931cf4930a80eb4cfce8b91d0",
            "05ca74c4de514d3d9fe088906d0a70ec",
            "74afa0acc8254b4bbe1c5d2b2ad1688c",
            "b0e894a6c4c241798a28adeee1c1e9ec",
            "eed9cfd2c71341fbaa06f762fb6bcb88",
            "f2533266dfc34598974a67639df64987",
            "be6641af8db94080b0542e3923aa028a",
            "25481b7a1ba842629ca497f0d61fcfa9",
            "32b27fcac302493f9c7386bd95a919df",
            "067b11dd959c44aabd58e2eaeac17603",
            "a3859ad210f143a78c29ef4c45fbb931",
            "d397ab76fa2b4685a12066b26ab38906",
            "657fa299680540709df91af1b21364a4",
            "0a0a4a16ffd34983bd6b278b98eded6f",
            "440694dcd1744a7597fb4dea53c56a54",
            "48d2733455fa415fb88fe908c46b05c3",
            "145190fb386c47e0bcedb6f6e3e9c132",
            "da8e29855712466aaad134aacabcd834",
            "a5f34bf2a928460f9ed6dc9f66fca8e9",
            "7dfe7d3570724378bf73aed8c518d8cb",
            "02dfa3aaae9b436d9301acb622dad293",
            "eadcc20970834dc8a1a460e0bc4b5ad4",
            "1f586c1e085c4bffaf3c14c96ba2d931",
            "216d0926898b449aafd1ab2236ee3b07",
            "abc672dfa4ed456aa2654853757dc681",
            "5df57d7421ab468bb361a1e03f760d66",
            "d48af84c284e403b8b0a397620089495",
            "d5c801b21e2b4f90b1384f9c93d09a97",
            "a421e30b4cdd496c91cfd0c1e36b772f",
            "80e1cfcf699e47dd99f7880e5956102f",
            "f39ca67517fb45b1a457d5e943fecafb",
            "c45667f66f5a4f898ceeef48beaf1ab9",
            "38efbed06d1c4f2d9abb4b47598de482",
            "ee2a632b968a412f94fe99d0ec4f30e9",
            "41189e1d71d949d3871b996284a2d158",
            "132eaeef073344bfa510e16d760948e8",
            "1018a34df4ed4d4aa151101f8a79897a",
            "857ec5ce26e446d5a02ffd31c8a5c6f2",
            "dbaaa1ae914c4a1d8bd8feb1d0eba0ce",
            "311b5fe061da4ac99d1659e200f02d53",
            "8e39b9d338b24b4ea115738a432f6931",
            "64f9c1b5a75645d6b290ef9a0d43c697",
            "075667c192c14d618b78ef7072bfa16c",
            "158f1faf6e974eebb6321586f4deaf5c",
            "ce1f1bc912f84dc1b151fcf808ed580d",
            "9e54efbc28b54c64a45c228e50a32bb1",
            "f56c5cedc9ac44db9f1e6787ff1e0b12",
            "390af216ec1943f28c05d7a0e44d6189",
            "1f0e385d5c47428a84b03448c8dc8003",
            "fd6675d188304e8a80c43d50b04b9bbb",
            "08c1a9d688614692859f486d8bdb41dc",
            "4e1f77fa5b5d4d7e8ad4abdd6aaafba2",
            "b4b7134dbd38413fbaac4183ae103146",
            "a4afcb70a45240029db6f1617820b983",
            "147cfce9c2bc43d583105ac4ae46485d",
            "6d7dbf2da2a5482fbffd7eb319d8fda3",
            "227891f28e4e477482d401c198e32349",
            "8afb8966b7974c7db838f16ba0626f00",
            "be3b5f9e94ac4d9f96ab3c34d89edf47",
            "3f01c431f9f743cd9a28944b3090e8f7",
            "ef10c661f662415d8995c6133ccd53ce",
            "726b714db6ad402f8329cf67bb6ed2b4"
          ]
        },
        "id": "cg3fiQOvmI3Q",
        "outputId": "e61cc533-a2d4-4a08-bd5c-1296fad1aa69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ad5e5af8704b0f8e44f58ab6226f21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/41.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9038797e5e0941f187a8161f17595d3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30abe82956fb4c7dad56816e24c2664f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31c06feb84bc44e586ea0fb44e14bcbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0a1df2f6b08469a8e6241dbda85f284"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5868fca753f24a5580cb9e506dd78ce8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25481b7a1ba842629ca497f0d61fcfa9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5f34bf2a928460f9ed6dc9f66fca8e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80e1cfcf699e47dd99f7880e5956102f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e39b9d338b24b4ea115738a432f6931"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e1f77fa5b5d4d7e8ad4abdd6aaafba2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-6.7b\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTSZntA1iUG"
      },
      "source": [
        "Post-processing on the model\n",
        "\n",
        "Applying some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability. I have also cast the output of the last layer in `float32` for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-gy-LxM0yAi"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOTr7B3NlM3"
      },
      "source": [
        "Applying LoRA\n",
        "\n",
        "Here comes the magic with peft. Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function from peft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W1j6lxaNnxC"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iwHGzKBN6wk",
        "outputId": "d6037833-4cff-47be-ea61-6071d0f26cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 8388608 || all params: 6666862592 || trainable%: 0.12582542214183376\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdjWif4CVXR6"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ_HCYruWIHU"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHYljmTjj5wX"
      },
      "source": [
        "Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDqJWba-tpnv",
        "outputId": "a56848e8-5f15-44ff-c865-cd74bd874b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1553: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1671: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Two things are infinite:  1. The universe  2. The number of zeroes in the number line  I'm not sure what the second one means.\n",
            "The number of zeroes in the number line is infinite.\n",
            "I'm not sure what the second\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(\"Two things are infinite: \", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"How far away is the Moon from the Earth? \", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=150)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_vRC4D1MwoK",
        "outputId": "3b5552e6-1225-4f32-fe41-7ddbe4b08575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " How far away is the Moon from the Earth? \n",
            "<s>.com has a great interactive map of the Moon's orbit:\n",
            "\n",
            "I'm not sure if this is the right place to ask this, but I'm curious.\n",
            "I've been reading a lot about the Moon lately, and I've been wondering how far away it is from the Earth.\n",
            "I know that it's about 238,000 miles away, but I'm wondering how far away it is from the Sun.\n",
            "\n",
            "It's<s>.com, not.com.\n",
            "\n",
            "I'm not sure if this is the right place to ask this, but I'm<s>.com, not.com.\n",
            "\n",
            "I'm not sure if this is the right place to ask this, but I'm.com, not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"There are two hardships:\", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=150)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr2oV_U3Rvwt",
        "outputId": "282490de-5962-4f87-f00d-325ee3f695f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " There are two hardships:  1. The first is that you have to be a member of the church.  2. The second is that you have to be a member of the church.\n",
            "I'm not a member of the church, but I'm not a member of the church.\n",
            "I'm not a member of the church, but I'm not a member of the church.\n",
            "I'm not a member of the church, but I'm not a member of the church.\n",
            "I'm not a member of the church, but I'm not a member of the church.\n",
            "I'm not a member of the church, but I'm not a member of the church.\n",
            "I'm not a member of the church, but I'm not a member\n"
          ]
        }
      ]
    }
  ]
}