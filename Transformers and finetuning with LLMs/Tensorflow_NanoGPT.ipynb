{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UMnD8rks80D_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LayerNormalization, MultiHeadAttention, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw9nTf1wFxhD",
        "outputId": "ccc88f19-698e-4b0a-c4b4-487fe3adbe59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YeNIbRcJHEE6"
      },
      "outputs": [],
      "source": [
        "# Setup Logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logger.info(f\"Starting Epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logger.info(f\"End of Epoch {epoch + 1}\")\n",
        "        for key, value in logs.items():\n",
        "            logger.info(f\"... {key}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "SKCM9Mad9gU0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect TPU and initialize\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print(\"TPU found and initialized!\")\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "    strategy = tf.distribute.get_strategy()  # Default strategy that works on CPU and single GPU\n",
        "\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDgaWaDp8Eiv",
        "outputId": "e03b87ea-e8b7-4259-9a3d-ab262e01bd3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU found and initialized!\n",
            "Number of replicas: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Look-ahead Mask\n",
        "def generate_lookahead_mask(size):\n",
        "    \"\"\"\n",
        "    Generate a lookahead mask to mask future tokens in a sequence.\n",
        "    \"\"\"\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "yUaqw9I8LThn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "wbcsH5gpaefW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(sz):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((sz, sz)), -1, 0)\n",
        "    return mask  # shape [sz, sz]\n"
      ],
      "metadata": {
        "id": "4qpdI0NGEdqn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(CustomMultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        assert self.embed_size % self.num_heads == 0\n",
        "\n",
        "        self.depth = embed_size // num_heads\n",
        "\n",
        "        self.wq = Dense(embed_size)\n",
        "        self.wk = Dense(embed_size)\n",
        "        self.wv = Dense(embed_size)\n",
        "\n",
        "        self.dense = Dense(embed_size)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, query, key, value, mask):\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        query = self.split_heads(self.wq(query), batch_size)\n",
        "        key = self.split_heads(self.wk(key), batch_size)\n",
        "        value = self.split_heads(self.wv(value), batch_size)\n",
        "\n",
        "        scaled_attention_logits = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(float(self.depth))\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, value)\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.embed_size))\n",
        "        return self.dense(concat_attention)\n"
      ],
      "metadata": {
        "id": "4GKfkoy5GYdx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "kukb9SqM85dy"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_size, heads, dropout_rate, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = CustomMultiHeadAttention(embed_size, heads)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.feed_forward = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(forward_expansion * embed_size, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_size)\n",
        "        ])\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x, mask = inputs\n",
        "        attn_output = self.attention(query=x, key=x, value=x, mask=mask)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.feed_forward(out1)\n",
        "        ffn_output = self.dropout(ffn_output)\n",
        "        return self.norm2(out1 + ffn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nanogpt(vocab_size, embed_size, num_layers, num_heads, ff_hidden_dim, dropout_rate, max_length):\n",
        "    inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    mask_input = tf.keras.layers.Input(shape=(max_length, max_length), dtype=tf.float32)\n",
        "\n",
        "    word_embeddings = tf.keras.layers.Embedding(vocab_size, embed_size)(inputs)\n",
        "    position_embeddings = tf.keras.layers.Embedding(max_length, embed_size)(tf.range(tf.shape(inputs)[1]))\n",
        "    x = word_embeddings + position_embeddings\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerBlock(embed_size, num_heads, dropout_rate, ff_hidden_dim)([x, mask_input])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, mask_input], outputs=outputs)\n"
      ],
      "metadata": {
        "id": "lgKGpofkFOLb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cv7_KlOw9Dj2"
      },
      "outputs": [],
      "source": [
        "def create_book_dataset(book_path, vocab, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Create a tf.data.Dataset from the text of a book.\n",
        "\n",
        "    Args:\n",
        "    - book_path (str): Path to the book's text file.\n",
        "    - vocab (dict): Dictionary mapping characters to integer tokens.\n",
        "    - sequence_length (int): Length of each input sequence.\n",
        "    - batch_size (int): Number of sequences in each batch.\n",
        "\n",
        "    Returns:\n",
        "    - dataset (tf.data.Dataset): Dataset of tokenized sequences.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the book text\n",
        "    with open(book_path, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Convert text to tokens\n",
        "    tokens = [vocab[char] for char in text if char in vocab]\n",
        "\n",
        "    # Create TensorFlow dataset from tokens\n",
        "    token_dataset = tf.data.Dataset.from_tensor_slices(tokens)\n",
        "\n",
        "    # Convert individual tokens to sequences of desired length\n",
        "    sequences = token_dataset.batch(sequence_length, drop_remainder=True)\n",
        "\n",
        "    # Batch sequences\n",
        "    dataset = sequences.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "eD9uLie99JJJ",
        "outputId": "ad1bbac8-1e8e-4c55-c1f1-4b79133ebae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-43e12fc1ad13>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_nanogpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_HEADS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFF_HIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-3870ae612820>\u001b[0m in \u001b[0;36mbuild_nanogpt\u001b[0;34m(vocab_size, embed_size, num_layers, num_heads, ff_hidden_dim, dropout_rate, max_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_hidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"transformer_block_25\" (type TransformerBlock).\n\nin user code:\n\n    File \"<ipython-input-50-64dab7545748>\", line 21, in call  *\n        ffn_output = self.feed_forward(out1)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ResourceExhaustedError: Exception encountered when calling layer 'sequential_24' (type Sequential).\n    \n    Failed to allocate request for 64.00MiB (67108864B) on device ordinal 0\n    \n    Call arguments received by layer 'sequential_24' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None, 256), dtype=float32)\n      • training=False\n      • mask=None\n\n\nCall arguments received by layer \"transformer_block_25\" (type TransformerBlock):\n  • inputs=['tf.Tensor(shape=(None, None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 512, 512), dtype=float32)']\n  • training=False"
          ]
        }
      ],
      "source": [
        "# Hyperparameters and data loading\n",
        "book_path = \"/content/drive/MyDrive/Harry Potter.txt\"  # Adjust path as needed\n",
        "text = open(book_path, 'r').read()\n",
        "vocab = {char: idx for idx, char in enumerate(sorted(set(text)))}\n",
        "sequence_length = 100\n",
        "batch_size = 16 * strategy.num_replicas_in_sync\n",
        "EMBED_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 2\n",
        "FF_HIDDEN_DIM = EMBED_SIZE\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_nanogpt(len(vocab), EMBED_SIZE, NUM_LAYERS, NUM_HEADS, FF_HIDDEN_DIM, DROPOUT_RATE, MAX_LENGTH)\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "    # Dataset\n",
        "    dataset = create_book_dataset(book_path, vocab, sequence_length, batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Training\n",
        "    EPOCHS = 10\n",
        "    logging_callback = LoggingCallback()\n",
        "    model.fit(dataset, epochs=EPOCHS, callbacks=[logging_callback])\n",
        "\n",
        "    # Define the path to save the model\n",
        "    save_path = \"/content/drive/MyDrive/tensorflow_nanogpt\"\n",
        "\n",
        "    # Save the entire model (architecture + weights)\n",
        "    model.save(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/NanoGPT_model\")\n"
      ],
      "metadata": {
        "id": "aLRL5RFlr6Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, vocab, inv_vocab, num_generate=500, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using a trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (tf.keras.Model): Trained model.\n",
        "    - start_string (str): Initial string to start the text generation.\n",
        "    - vocab (dict): Dictionary mapping characters to integer tokens.\n",
        "    - inv_vocab (dict): Dictionary mapping integer tokens back to characters.\n",
        "    - num_generate (int): Number of characters to generate.\n",
        "    - temperature (float): Controls the randomness of the output. Higher value is more random.\n",
        "\n",
        "    Returns:\n",
        "    - generated_text (str): Generated text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert start string to tokens\n",
        "    input_eval = [vocab[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    generated_text = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Update the input to the model for the next token prediction\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        generated_text.append(inv_vocab[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(generated_text)\n"
      ],
      "metadata": {
        "id": "fTiHzABzajLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_vocab = {v: k for k, v in vocab.items()}  # Inverse vocabulary mapping integer tokens to characters\n",
        "start_string = \"Once upon a time\"  # You can choose any starting string\n",
        "generated_output = generate_text(loaded_model, start_string, vocab, inv_vocab, num_generate=1000, temperature=0.7)\n",
        "\n",
        "print(generated_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "9LRF1_uwzFSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_vocab = {v: k for k, v in vocab.items()}  # Inverse vocabulary mapping integer tokens to characters\n",
        "\n",
        "question = \"Who is the main protagonist in the book?\"\n",
        "prompt = f\"Answering a question about the book: {question}\"\n",
        "answer = generate_text(loaded_model, prompt, vocab, inv_vocab, num_generate=150, temperature=0.7)\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "M96PRrchavi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}