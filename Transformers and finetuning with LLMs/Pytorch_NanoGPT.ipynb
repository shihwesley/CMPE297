{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UMnD8rks80D_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw9nTf1wFxhD",
        "outputId": "a39df72e-77e6-4c10-9751-12277cbb2cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YeNIbRcJHEE6"
      },
      "outputs": [],
      "source": [
        "# Setup Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Look-ahead Mask\n",
        "def generate_lookahead_mask(sequence_length):\n",
        "    mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
        "    return mask.masked_fill(mask==1, float('-inf'))"
      ],
      "metadata": {
        "id": "yUaqw9I8LThn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "wbcsH5gpaefW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kukb9SqM85dy"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.attention = nn.MultiheadAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        # Transpose the first two dimensions\n",
        "        value = value.transpose(0, 1)\n",
        "        key = key.transpose(0, 1)\n",
        "        query = query.transpose(0, 1)\n",
        "        if value.size(2) != self.embed_size or key.size(2) != self.embed_size or query.size(2) != self.embed_size:\n",
        "          logger.error(\"Input tensors have incorrect dimension!\")\n",
        "          raise ValueError(\"The dimensions of value, key, and query tensors must match the specified embed_size.\")\n",
        "\n",
        "        try:\n",
        "          attention, _ = self.attention(query, key, value, attn_mask=mask, need_weights=False)\n",
        "          # Transpose the first two dimensions back\n",
        "          attention = attention.transpose(0, 1)\n",
        "          x = self.dropout(self.norm1(attention + query.transpose(0, 1)))\n",
        "          forward = self.feed_forward(x)\n",
        "          out = self.dropout(self.norm2(forward + x))\n",
        "        except Exception as e:\n",
        "          logger.error(f\"Error occurred in TransformerBlock: {str(e)}\")\n",
        "          raise\n",
        "\n",
        "        return out\n",
        "\n",
        "class NanoGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion=4, dropout=0.5, max_length=512):\n",
        "        super(NanoGPT, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        if seq_length > self.position_embedding.num_embeddings:\n",
        "            logger.error(\"Input sequence length exceeds maximum allowed length!\")\n",
        "            raise ValueError(\"Input sequence length exceeds the maximum allowed length for positional embeddings.\")\n",
        "\n",
        "        try:\n",
        "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "            out = self.word_embedding(x) + self.position_embedding(positions)\n",
        "            for layer in self.transformer_blocks:\n",
        "                out = layer(out, out, out, mask)\n",
        "            out = self.fc_out(out)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error occurred in NanoGPT: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cv7_KlOw9Dj2"
      },
      "outputs": [],
      "source": [
        "class BookDataset(Dataset):\n",
        "    def __init__(self, file_path, vocab, sequence_length):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.text = f.read()\n",
        "        self.text = self.text.replace('\\n', ' ')\n",
        "        self.vocab = vocab\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.text[idx:idx+self.sequence_length]\n",
        "        input_sequence = torch.tensor([self.vocab[char] for char in sequence[:-1]], dtype=torch.long)\n",
        "        target_sequence = torch.tensor([self.vocab[char] for char in sequence[1:]], dtype=torch.long)\n",
        "        return input_sequence, target_sequence\n",
        "\n",
        "# Create Vocabulary from the Book Text\n",
        "with open(\"/content/drive/MyDrive/Harry Potter.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "unique_chars = set(text)\n",
        "vocab = {char: idx for idx, char in enumerate(unique_chars)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD9uLie99JJJ",
        "outputId": "59dcce49-b0e2-49af-cae3-9fb408a07855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 6,649,937 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NanoGPT(vocab_size=len(vocab), embed_size=512, num_layers=2, heads=2, device=device).to(device)\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters.\")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# DataLoader\n",
        "dataset = BookDataset(\"/content/drive/MyDrive/Harry Potter.txt\", vocab, sequence_length=100)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        num_heads = 2  # Make sure this matches the number of heads you're using in the MultiheadAttention module\n",
        "        mask = generate_lookahead_mask(data.shape[1]).unsqueeze(0).expand(num_heads * data.shape[0], -1, -1).to(device)\n",
        "\n",
        "        # Forward\n",
        "        try:\n",
        "            outputs = model(data, mask)\n",
        "            loss = loss_fn(outputs.view(-1, outputs.shape[2]), targets.view(-1))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during forward pass at epoch {epoch}, batch {batch_idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Backward\n",
        "        try:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during backward pass at epoch {epoch}, batch {batch_idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Print loss occasionally\n",
        "        if batch_idx % 100 == 0:\n",
        "            logger.info(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} Loss: {loss.item()}\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/\"  # Adjust path as needed\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, initial_text, max_length, vocab, device):\n",
        "    # Convert the initial text to tensor\n",
        "    initial_text = [vocab[char] for char in initial_text]\n",
        "    initial_tensor = torch.tensor(initial_text).unsqueeze(0).to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length - len(initial_text)):\n",
        "            # Generate a mask for the current sequence\n",
        "            mask = generate_lookahead_mask(initial_tensor.shape[1]).to(device)\n",
        "\n",
        "            # Get the model's predictions\n",
        "            outputs = model(initial_tensor, mask)\n",
        "            predictions = outputs[:, -1, :]  # Only get the prediction for the last character\n",
        "\n",
        "            # Choose the character with the highest probability as the next character\n",
        "            _, next_char_idx = predictions.topk(1, dim=-1)\n",
        "            initial_tensor = torch.cat([initial_tensor, next_char_idx], dim=1)  # Add the new character to the sequence\n",
        "\n",
        "    # Convert the tensor of character indices back to string\n",
        "    generated_text = ''.join([list(vocab.keys())[list(vocab.values()).index(idx)] for idx in initial_tensor[0].tolist()])\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "fTiHzABzajLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you've defined the NanoGPT class in your script\n",
        "model = NanoGPT(vocab_size=len(vocab), embed_size=512, num_layers=2, heads=2, device=device)  # Adjust parameters as needed\n",
        "\n",
        "# Load the state dictionary\n",
        "model_path = \"/content/drive/MyDrive/\"  # Adjust path as needed\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)  # Move model to the desired device (e.g., CUDA)\n"
      ],
      "metadata": {
        "id": "9LRF1_uwzFSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_text = \"once upon a\"  # Starting text\n",
        "generated_length = 100  # The total length of the generated text\n",
        "\n",
        "generated = generate_text(model, initial_text, generated_length, vocab, device)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "U1IqTcB9amIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the main protagonist in the book?\"\n",
        "prompt = f\"Answering a question about the book: {question}\"\n",
        "answer = generate_text(model, prompt, 150, vocab, device)  # 150 is just a chosen length\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "M96PRrchavi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}